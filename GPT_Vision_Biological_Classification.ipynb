{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Angsumi/GPT_VISION_FOR_Animal_Classification/blob/main/GPT_Vision_Biological_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "unique-id",
      "metadata": {
        "id": "unique-id"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from wordcloud import WordCloud\n",
        "import seaborn as sns\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FWH0zv4FKa1T"
      },
      "outputs": [],
      "source": [
        "# Load Excel sheets into dataframes\n",
        "\n",
        "xls = pd.ExcelFile('/mnt/data/Species List.xlsx')\n",
        "input_df = xls.parse('Input')\n",
        "output_df = xls.parse('Output')\n"
      ],
      "id": "FWH0zv4FKa1T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEbPDzjrKa1Y"
      },
      "outputs": [],
      "source": [
        "# Data Cleaning Function\n",
        "\n",
        "def clean_dataframe(df):\n",
        "    df.columns = [col.capitalize().strip() for col in df.columns]\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            if col == 'Species':\n",
        "                df[col] = df[col].str.split().str[1].str.capitalize().fillna(df[col].str.split().str[0].str.capitalize())\n",
        "            else:\n",
        "                df[col] = df[col].str.split().str[0].str.capitalize().str.strip()\n",
        "    return df\n",
        "\n",
        "input_df_cleaned = clean_dataframe(input_df.copy())\n",
        "output_df_cleaned = clean_dataframe(output_df.copy())\n"
      ],
      "id": "hEbPDzjrKa1Y"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-WvxSRMuKa1d"
      },
      "outputs": [],
      "source": [
        "# Pie Diagrams for Empty Predictions\n",
        "\n",
        "fig, axes = plt.subplots(1, output_df_cleaned.shape[1]-1, figsize=(20, 4))\n",
        "for idx, col in enumerate(output_df_cleaned.columns[1:]):\n",
        "    counts = [\"Predicted\", \"Unable to predict\"]\n",
        "    values = [output_df_cleaned[col].notna().sum(), output_df_cleaned[col].isna().sum()]\n",
        "    axes[idx].pie(values, labels=counts, autopct='%1.1f%%', startangle=90, colors=['#4CAF50', '#FFC107'])\n",
        "    axes[idx].set_title(col)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "-WvxSRMuKa1d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZESCp3YAKa1g"
      },
      "outputs": [],
      "source": [
        "# Accuracy Calculation\n",
        "\n",
        "accuracies = {}\n",
        "for col in input_df_cleaned.columns[1:]:\n",
        "    correct_predictions = (input_df_cleaned[col] == output_df_cleaned[col]).sum()\n",
        "    total_entries = input_df_cleaned.shape[0]\n",
        "    accuracy = correct_predictions / total_entries\n",
        "    accuracies[col] = accuracy\n",
        "accuracies\n"
      ],
      "id": "ZESCp3YAKa1g"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "laQjYcSFKa1i"
      },
      "outputs": [],
      "source": [
        "# Word Cloud for Misclassifications\n",
        "\n",
        "def get_misclassified_terms(input_df, output_df, rank):\n",
        "    misclassified = input_df[input_df[rank] != output_df[rank]]\n",
        "    return misclassified[rank].value_counts().to_dict()\n",
        "\n",
        "ranks = ['Phylum', 'Class', 'Order', 'Family']\n",
        "fig, axes = plt.subplots(1, len(ranks), figsize=(20, 5))\n",
        "for idx, rank in enumerate(ranks):\n",
        "    misclassified_terms = get_misclassified_terms(input_df_cleaned, output_df_cleaned, rank)\n",
        "    wc = WordCloud(background_color='white', width=400, height=400).generate_from_frequencies(misclassified_terms)\n",
        "    axes[idx].imshow(wc, interpolation='bilinear')\n",
        "    axes[idx].axis('off')\n",
        "    axes[idx].set_title(f'Misclassified {rank}')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "id": "laQjYcSFKa1i"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfSqxrZlKa1m"
      },
      "outputs": [],
      "source": [
        "# Precision, Recall, and F1-Score Calculation\n",
        "\n",
        "precision_list = []\n",
        "recall_list = []\n",
        "f1_list = []\n",
        "accuracy_list = []\n",
        "\n",
        "for col in input_df_cleaned.columns[1:]:\n",
        "    mask = input_df_cleaned[col].notna() & output_df_cleaned[col].notna()\n",
        "    y_true = input_df_cleaned[mask][col]\n",
        "    y_pred = output_df_cleaned[mask][col]\n",
        "\n",
        "    precision_list.append(precision_score(y_true, y_pred, average='weighted'))\n",
        "    recall_list.append(recall_score(y_true, y_pred, average='weighted'))\n",
        "    f1_list.append(f1_score(y_true, y_pred, average='weighted'))\n",
        "    accuracy_list.append((y_true == y_pred).mean())\n",
        "\n",
        "metrics_df = pd.DataFrame({\n",
        "    'Classification Level': input_df_cleaned.columns[1:],\n",
        "    'Accuracy': accuracy_list,\n",
        "    'Precision': precision_list,\n",
        "    'Recall': recall_list,\n",
        "    'F1-Score': f1_list\n",
        "})\n",
        "metrics_df\n"
      ],
      "id": "vfSqxrZlKa1m"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy for each taxonomic level considering and not considering null values\n",
        "accuracies_with_null = []\n",
        "accuracies_without_null = []\n",
        "\n",
        "columns = output_df_cleaned.columns[1:]\n",
        "\n",
        "for col in columns:\n",
        "    total_values = output_df_cleaned.shape[0]\n",
        "\n",
        "    # Correct predictions (both dataframes have the same non-null value)\n",
        "    correct_predictions = (input_df_cleaned[col] == output_df_cleaned[col]).sum()\n",
        "\n",
        "    # Not considering null values: only consider rows where both dataframes have non-null values\n",
        "    not_null_rows = (input_df_cleaned[col].notna() & output_df_cleaned[col].notna()).sum()\n",
        "\n",
        "    # Calculate accuracies\n",
        "    accuracy_with_null = correct_predictions / total_values\n",
        "    accuracy_without_null = correct_predictions / not_null_rows if not_null_rows != 0 else 0\n",
        "\n",
        "    accuracies_with_null.append(accuracy_with_null)\n",
        "    accuracies_without_null.append(accuracy_without_null)\n",
        "\n",
        "# Plotting the accuracies\n",
        "fig, ax = plt.subplots(figsize=(12, 6))\n",
        "\n",
        "# Bar positions\n",
        "bar_width = 0.35\n",
        "index = range(len(columns))\n",
        "\n",
        "bar1 = ax.bar(index, accuracies_with_null, bar_width, label='With Null Values', color='b')\n",
        "bar2 = ax.bar([i + bar_width for i in index], accuracies_without_null, bar_width, label='Without Null Values', color='r')\n",
        "\n",
        "# Labeling the plot\n",
        "ax.set_xlabel('Taxonomic Level')\n",
        "ax.set_ylabel('Accuracy')\n",
        "ax.set_title('Accuracy at Each Taxonomic Level')\n",
        "ax.set_xticks([i + bar_width / 2 for i in index])\n",
        "ax.set_xticklabels(columns, rotation=45)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "dPe8SvGNK0v1"
      },
      "id": "dPe8SvGNK0v1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a confusion matrix for classification level (Phylusm, Class, order).\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import seaborn as sns\n",
        "\n",
        "# Function to plot confusion matrix for a given rank\n",
        "def plot_confusion_matrix(rank, input_df, output_df):\n",
        "    # Filter out rows with NaN values in the considered rank for both input and output dataframes\n",
        "    mask = input_df[rank].notna() & output_df[rank].notna()\n",
        "    y_true = input_df[mask][rank]\n",
        "    y_pred = output_df[mask][rank]\n",
        "\n",
        "    # Compute confusion matrix\n",
        "    labels = sorted(list(set(y_true).union(set(y_pred))))\n",
        "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "\n",
        "    # Plot\n",
        "    fig, ax = plt.subplots(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels, ax=ax)\n",
        "    ax.set_ylabel('True Label')\n",
        "    ax.set_xlabel('Predicted Label')\n",
        "    ax.set_title(f'Confusion Matrix for {rank}')\n",
        "    plt.show()\n",
        "\n",
        "# Plot confusion matrices for 'Phylum', 'Class', and 'Order'\n",
        "for rank in ['Phylum', 'Class', 'Order']:\n",
        "    plot_confusion_matrix(rank, input_df_cleaned, output_df_cleaned)\n"
      ],
      "metadata": {
        "id": "3iYWaOK-K2CZ"
      },
      "id": "3iYWaOK-K2CZ",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "orig_nbformat": 2,
    "file_extension": ".ipynb",
    "mimetype": "text/x-ipynb",
    "name": "ipython",
    "version": 3,
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}